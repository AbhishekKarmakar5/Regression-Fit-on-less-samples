{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37759f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff32272",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.read_csv(\"Product Wise ARR Report - Sheet5 (1).csv\").drop(['Subscription Product ARR (converted)'], axis=1)\n",
    "product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6bdddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = product_df.groupby(['Account Name', 'Product Name']).apply(lambda x: pd.Series({'Total Quantity': x['Quantity'].sum(),\n",
    "    'Weighted Average Price': (x['Quantity'] * x['Offered Price (converted)']).sum() / x['Quantity'].sum() }))\n",
    "\n",
    "product_df = product_df.reset_index()\n",
    "product_df = product_df.drop(['Account Name'], axis=1)\n",
    "product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb8826",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df['Product Name'] = product_df['Product Name'].str.replace('Training Essentials ', 'Training Essentials', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df.groupby(['Product Name']).size().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62bd5e2",
   "metadata": {},
   "source": [
    "# Polynomial Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3250a301",
   "metadata": {},
   "source": [
    "# L2 - Ridge Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a55a4c",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning considering the number of variables as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ba616f",
   "metadata": {},
   "source": [
    "# Generalized Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeda920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('images', exist_ok=True)\n",
    "os.makedirs('SVR_models', exist_ok=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform, expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "polynomial_degree_df = pd.DataFrame()\n",
    "\n",
    "for grouped_value, grouped_df in product_df.groupby(['Product Name']):\n",
    "    X = grouped_df['Total Quantity'].values.reshape(-1, 1)\n",
    "    y = grouped_df['Weighted Average Price']\n",
    "\n",
    "    # Splitting the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    # Setting up SVR model\n",
    "    svr_model = SVR()\n",
    "\n",
    "    # Defining the parameter distribution for tuning\n",
    "    param_dist = {\n",
    "        'C': [0.1, 1, 10, 100],  # Discrete values for C\n",
    "        'epsilon': [0.01, 0.1, 0.5, 1],  # Discrete values for epsilon\n",
    "        'kernel': ['linear', 'poly', 'rbf']  # Can limit to fewer options\n",
    "    }\n",
    "    if 'poly' in param_dist['kernel']:\n",
    "        param_dist['degree'] = [2, 3]  # Limiting the degree for polynomial kernel\n",
    "\n",
    "    # Setting up RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(svr_model, param_distributions=param_dist, \n",
    "                                       n_iter=30, cv=5, random_state=0, n_jobs=-1)\n",
    "\n",
    "    # Running the random search to find the best hyperparameters\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Training the model using the best parameters\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Evaluating the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'R² Score: {r2}')\n",
    "    \n",
    "    # Saving the model\n",
    "    model_folder_path = 'SVR_models'\n",
    "    if not os.path.exists(model_folder_path):\n",
    "        os.makedirs(model_folder_path)\n",
    "    model_filename = os.path.join(model_folder_path, f'SVR_model_{grouped_value}.joblib')\n",
    "    joblib.dump(best_model, model_filename)    \n",
    "\n",
    "    # Plotting\n",
    "    plt.scatter(X_train, y_train, color='green', label='Training Data')\n",
    "    plt.scatter(X_test, y_test, color='red', label='Testing Data')\n",
    "    plt.scatter(X_test, y_pred, color='blue', label='Predicted Values')\n",
    "\n",
    "    plt.xlabel('Total Quantity')\n",
    "    plt.ylabel('Weighted Average Price')\n",
    "    plt.title(f'SVR Model for {grouped_value}')    \n",
    "    plt.legend()\n",
    "    plot_file_path = f'images/{grouped_value}_SVR_plot.png'\n",
    "    plt.savefig(plot_file_path)    \n",
    "    plt.close()\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Product Name\": [grouped_value],\n",
    "        \"Mean Squared Error\": [mse],\n",
    "        \"R² Score\": [r2],\n",
    "        \"Best Parameters\": [random_search.best_params_],\n",
    "        \"Plot Image Path\": [plot_file_path]\n",
    "    })\n",
    "\n",
    "    # Append this DataFrame to the main DataFrame\n",
    "    polynomial_degree_df = pd.concat([polynomial_degree_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e434b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_degree_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'Product Name' and finding the index of minimum 'Mean Squared Error' for each group\n",
    "idx = polynomial_degree_df.groupby('Product Name')['Mean Squared Error'].idxmin()\n",
    "best_models_df = polynomial_degree_df.loc[idx]\n",
    "best_models_df.reset_index(drop=True, inplace=True)\n",
    "best_models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeefec98",
   "metadata": {},
   "source": [
    "# Save the trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920bbdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "os.makedirs('Models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ba903",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in best_models_df.iterrows():\n",
    "    product_name = row['Product Name']\n",
    "    degree = row['Degree']\n",
    "    alpha = row['Best Alpha']\n",
    "    grouped_df = product_df[product_df['Product Name'] == product_name]\n",
    "\n",
    "    X = grouped_df['Total Quantity'].values.reshape(-1, 1)\n",
    "    y = grouped_df['Weighted Average Price']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    model = make_pipeline(StandardScaler(), PolynomialFeatures(degree=degree), Ridge(alpha=alpha, random_state=0))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the model\n",
    "    filename = f'Models/model_{product_name}_degree_{degree}_alpha_{alpha:.4f}.joblib'\n",
    "    joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd4e6f",
   "metadata": {},
   "source": [
    "# Prediction Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "def predict_price(product_name, total_quantity):\n",
    "    models_dir = 'Models'\n",
    "\n",
    "    model_file = None\n",
    "    for file in os.listdir(models_dir):\n",
    "        if product_name in file:\n",
    "            model_file = file\n",
    "            break\n",
    "\n",
    "    if model_file is None:\n",
    "        return f\"No model found for product: {product_name}\"\n",
    "\n",
    "    # Load the model\n",
    "    model_path = os.path.join(models_dir, model_file)\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    # Make a prediction\n",
    "    predicted_price = model.predict([[total_quantity]])\n",
    "\n",
    "    return predicted_price[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa11cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = 'Virtual Role-Play (Missions + Quick Update)' \n",
    "total_quantity = 1700\n",
    "predicted_price = predict_price(product_name, total_quantity)\n",
    "print(f\"Predicted Price: {predicted_price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d26aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df[product_df['Product Name'] == product_name].sort_values(['Weighted Average Price'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
